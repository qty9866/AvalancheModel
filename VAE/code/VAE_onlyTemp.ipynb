{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sensible_heat', 'latent_heat', 'ground_temperature', 'rain_energy',\n",
      "       'outgoing_long_wave_radiation', 'incoming_long_wave_radiation',\n",
      "       'net_long_wave_radiation', 'reflected_short_wave_radiation',\n",
      "       'incoming_short_wave_radiation', 'net_short_wave_radiation',\n",
      "       'parametrized_albedo', 'incoming_short_wave_on_horizontal',\n",
      "       'direct_incoming_short_wave', 'diffuse_incoming_short_wave',\n",
      "       'air_temperature', 'surface_temperature(mod)', 'bottom_temperature',\n",
      "       'relative_humidity', 'wind_velocity', 'wind_velocity_drift',\n",
      "       'wind_direction', 'solid_precipitation_rate', 'snow_height(mod)',\n",
      "       'snow_height(meas)', 'hoar_size', '24h_height_of_new_snow',\n",
      "       '3d_sum_of_daily_height_of_new_snow', 'snow_water_equivalent',\n",
      "       'total_amount_of_water', 'rain_rate', 'virtual_lysimeter',\n",
      "       'sublimation_mass', 'evaporated_mass', 'stability_class', 'z_Sdef',\n",
      "       'deformation_rate_stability_index', 'z_Sn38', 'natural_stability_index',\n",
      "       'z_Sk38', 'Sk38_skier_stability_index', 'z_SSI',\n",
      "       'structural_stability_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/home/develop/VAE/data/csv/final.csv\"\n",
    "\n",
    "# 读取CSV文件并打印列名\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理文件时出错: \"['surface_temperature(meas)'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "input_file = \"/home/develop/VAE/data/csv/final.csv\"\n",
    "output_file = \"/home/develop/VAE/data/csv/final_temp.csv\"\n",
    "\n",
    "# 温度相关列\n",
    "temperature_columns = [\n",
    "    \"ground_temperature\",\n",
    "    \"surface_temperature(mod)\",\n",
    "    \"air_temperature\",\n",
    "    \"bottom_temperature\"\n",
    "]\n",
    "\n",
    "# 读取CSV文件并筛选温度相关列\n",
    "try:\n",
    "    data = pd.read_csv(input_file)\n",
    "    temp_data = data[temperature_columns]\n",
    "    temp_data.to_csv(output_file, index=False)\n",
    "    print(f\"温度相关列已保存到文件: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"处理文件时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件行数1186,列数4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/develop/VAE/data/csv/final_temp.csv')\n",
    "x,y=data.shape\n",
    "print(f\"文件行数{x},列数{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data shape: (1186, 4)\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_794056/3041667768.py\", line 59, in vae_loss  *\n        decoder_output, z_mean, z_log_var = y_pred\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m vae, vae_loss \u001b[39m=\u001b[39m build_vae(input_dim, latent_dim, kl_loss_weight)\n\u001b[1;32m     81\u001b[0m vae\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m), loss\u001b[39m=\u001b[39mvae_loss)\n\u001b[0;32m---> 83\u001b[0m history \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     84\u001b[0m     X_train, [X_train, np\u001b[39m.\u001b[39;49mzeros((X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], latent_dim)), np\u001b[39m.\u001b[39;49mzeros((X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], latent_dim))],\n\u001b[1;32m     85\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     86\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     87\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test, [X_test, np\u001b[39m.\u001b[39;49mzeros((X_test\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], latent_dim)), np\u001b[39m.\u001b[39;49mzeros((X_test\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], latent_dim))]),\n\u001b[1;32m     88\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining complete for latent_dim=\u001b[39m\u001b[39m{\u001b[39;00mlatent_dim\u001b[39m}\u001b[39;00m\u001b[39m, batch_size=\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, epochs=\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_794056/3041667768.py\", line 59, in vae_loss  *\n        decoder_output, z_mean, z_log_var = y_pred\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# 数据读取\n",
    "data = pd.read_csv('/home/develop/VAE/data/csv/final_temp.csv')\n",
    "X = data.values\n",
    "\n",
    "# 数据集划分\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "\n",
    "input_dim = X.shape[1] \n",
    "\n",
    "# 超参数网格\n",
    "latent_dims = [1, 3, 5, 7]  # 潜在空间维度\n",
    "batch_sizes = [8, 16, 32]\n",
    "epochs_list = [10, 20, 30]\n",
    "kl_loss_weight = 1.0  \n",
    "\n",
    "save_dir = \"/home/develop/VAE/Result/Temp_pic/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "log_file = \"/home/develop/VAE/Result/Temp_train.log\" \n",
    "\n",
    "def build_vae(input_dim, latent_dim):\n",
    "    # 编码器\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(16, activation=\"relu\")(input_layer)\n",
    "    encoder = layers.Dense(8, activation=\"relu\")(encoder)\n",
    "    z_mean = layers.Dense(latent_dim)(encoder)\n",
    "    z_log_var = layers.Dense(latent_dim)(encoder)\n",
    "\n",
    "    # 采样层\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # 解码器\n",
    "    decoder = layers.Dense(8, activation=\"relu\")(z)\n",
    "    decoder = layers.Dense(16, activation=\"relu\")(decoder)\n",
    "    decoder_output = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "\n",
    "    vae = Model(input_layer, [decoder_output, z_mean, z_log_var])\n",
    "\n",
    "    def vae_loss(y_true, y_pred):\n",
    "        decoder_output = y_pred[0]\n",
    "        z_mean = y_pred[1]\n",
    "        z_log_var = y_pred[2]\n",
    "        reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        reconstruction_loss = reconstruction_loss_fn(y_true, decoder_output)\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        return reconstruction_loss + kl_loss_weight * tf.reduce_mean(kl_loss)\n",
    "\n",
    "    return vae, vae_loss\n",
    "\n",
    "# 开始训练和记录日志\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"Training Log\\n\")\n",
    "    log.write(\"Parameters: latent_dim, batch_size, epochs\\n\")\n",
    "    log.write(\"Results: reconstruction_error_threshold, anomalies_detected, training_time\\n\")\n",
    "    log.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                # 构建模型\n",
    "                vae, vae_loss = build_vae(input_dim, latent_dim)\n",
    "                vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=vae_loss)\n",
    "\n",
    "                # 训练模型\n",
    "                start_time = datetime.now()\n",
    "                history = vae.fit(\n",
    "                    X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=0\n",
    "                )\n",
    "                training_time = datetime.now() - start_time\n",
    "\n",
    "                # 绘制训练损失\n",
    "                plt.figure()\n",
    "                plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "                plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.legend()\n",
    "                plt.title(f\"Loss (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}loss_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "\n",
    "                # 异常检测\n",
    "                X_pred = vae.predict(X_test, verbose=0)[0]\n",
    "                reconstruction_error = np.mean(np.square(X_test - X_pred), axis=1)\n",
    "                threshold = np.percentile(reconstruction_error, 95)\n",
    "                anomalies = reconstruction_error > threshold\n",
    "\n",
    "                # 绘制重构误差分布\n",
    "                plt.figure()\n",
    "                plt.hist(reconstruction_error, bins=50)\n",
    "                plt.xlabel(\"Reconstruction Error\")\n",
    "                plt.ylabel(\"Number of Samples\")\n",
    "                plt.title(f\"Error Dist. (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}error_dist_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "                log.write(f\"latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\\n\")\n",
    "                log.write(f\"reconstruction_error_threshold={threshold:.4f}, anomalies_detected={np.sum(anomalies)}, training_time={training_time}\\n\")\n",
    "                log.write(\"-\" * 80 + \"\\n\")\n",
    "                print(f\"Params: latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                print(f\"Reconstruction Error Threshold: {threshold:.4f}\")\n",
    "                print(f\"Anomalies detected: {np.sum(anomalies)}\")\n",
    "                print(f\"Training Time: {training_time}\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

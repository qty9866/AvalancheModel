{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smet文件转换为csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_folder = '/home/develop/Station/data/origin'\n",
    "output_folder = '/home/develop/Station/data/csvwithHeader/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "smet_files = [f for f in os.listdir(input_folder) if f.endswith('.smet')]\n",
    "\n",
    "for smet_file in smet_files:\n",
    "    smet_file_path = os.path.join(input_folder, smet_file)\n",
    "    data = pd.read_csv(smet_file_path, sep='\\t', header=None)\n",
    "    output_csv_path = os.path.join(output_folder, smet_file.replace('.smet', '.csv'))\n",
    "    data.to_csv(output_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除头部Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理并保存文件: station.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_dir = '/home/develop/Station/data/csvwithHeader/'\n",
    "output_dir = '/home/develop/Station/data/csv/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_dir, filename)     \n",
    "        with open(input_filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        start_index = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if '[DATA]' in line:\n",
    "                start_index = i + 1  \n",
    "                break\n",
    "\n",
    "        \n",
    "        if start_index is not None:\n",
    "            data_to_keep = lines[start_index:]\n",
    "            output_filepath = os.path.join(output_dir, filename)\n",
    "            with open(output_filepath, 'w') as output_file:\n",
    "                output_file.writelines(data_to_keep)\n",
    "            print(f\"处理并保存文件: {filename}\")\n",
    "        else:\n",
    "            print(f\"文件 {filename} 中没有找到 [DATA] 标签\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同列之间用逗号隔开同时将内容转换为数值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 station.csv 已成功转换，并保留了时间戳列。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_340351/2389777297.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定CSV文件夹路径\n",
    "folder_path = '/home/develop/Station/data/csv/'\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "        timestamps = data.iloc[:, 0]\n",
    "        numeric_data = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "        processed_data = pd.concat([timestamps, numeric_data], axis=1)\n",
    "        processed_data.to_csv(file_path, index=False, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列数匹配，表头将被添加到文件中: station.csv\n",
      "表头已成功添加到 station.csv。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = '/home/develop/Station/data/csv/'\n",
    "header = [\n",
    "    \"timestamp\",\"sensible_heat\", \"latent_heat\", \"ground_heat\", \"ground_temperature\", \n",
    "    \"ground_heat_at_soil_interface\", \"rain_energy\", \"outgoing_long_wave_radiation\", \n",
    "    \"incoming_long_wave_radiation\", \"net_long_wave_radiation\", \"reflected_short_wave_radiation\", \n",
    "    \"incoming_short_wave_radiation\", \"net_short_wave_radiation\", \"parametrized_albedo\", \n",
    "    \"measured_albedo\", \"incoming_short_wave_on_horizontal\", \"direct_incoming_short_wave\", \n",
    "    \"diffuse_incoming_short_wave\", \"air_temperature\", \"surface_temperature(mod)\", \n",
    "    \"surface_temperature(meas)\", \"bottom_temperature\", \"relative_humidity\", \"wind_velocity\", \n",
    "    \"wind_velocity_drift\", \"wind_direction\", \"solid_precipitation_rate\", \"snow_height(mod)\", \n",
    "    \"snow_height(meas)\", \"hoar_size\", \"24h_wind_drift\", \"24h_height_of_new_snow\", \n",
    "    \"3d_sum_of_daily_height_of_new_snow\", \"snow_water_equivalent\", \"total_amount_of_water\", \n",
    "    \"erosion_mass_loss\", \"rain_rate\", \"virtual_lysimeter\", \n",
    "    \"virtual_lysimeter_under_the_soil\", \"sublimation_mass\", \"evaporated_mass\", \n",
    "    \"temperature@0.25m\", \"temperature@0.5m\", \"temperature@1m\", \"temperature@-0.25m\", \n",
    "    \"temperature@-0.1m\", \"profile_type\", \"stability_class\", \"z_Sdef\", \n",
    "    \"deformation_rate_stability_index\", \"z_Sn38\", \"natural_stability_index\", \"z_Sk38\", \n",
    "    \"Sk38_skier_stability_index\", \"z_SSI\", \"structural_stability_index\", \"z_S5\", \n",
    "    \"stability_index_5\"\n",
    "]\n",
    "\n",
    "for file_name in os.listdir(folder_path):    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "  \n",
    "    if not file_path.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(file_path, header=None, sep=',')\n",
    "        \n",
    "        if len(data.columns) == len(header):\n",
    "            print(f\"列数匹配，表头将被添加到文件中: {file_name}\")\n",
    "            data.columns = header\n",
    "            data.to_csv(file_path, index=False, sep=',')\n",
    "            print(f\"表头已成功添加到 {file_name}。\")\n",
    "        else:\n",
    "            print(f\"列数不匹配！文件: {file_name}，实际列数为 {len(data.columns)}，表头参数个数为 {len(header)}。\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file_name} 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历文件，找到值不变的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列 'ground_heat' 的所有值相同，值为: -999\n",
      "列 'measured_albedo' 的所有值相同，值为: -999\n",
      "列 'surface_temperature(meas)' 的所有值相同，值为: -999\n",
      "列 '24h_wind_drift' 的所有值相同，值为: 0.0\n",
      "列 'virtual_lysimeter_under_the_soil' 的所有值相同，值为: -999\n",
      "列 'temperature@0.25m' 的所有值相同，值为: -999\n",
      "列 'temperature@0.5m' 的所有值相同，值为: -999\n",
      "列 'temperature@1m' 的所有值相同，值为: -999\n",
      "列 'temperature@-0.25m' 的所有值相同，值为: -999\n",
      "列 'temperature@-0.1m' 的所有值相同，值为: -999\n",
      "列 'profile_type' 的所有值相同，值为: -1.0\n",
      "列 'z_S5' 的所有值相同，值为: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "# 遍历每一列\n",
    "for col in data.columns:\n",
    "    # 检查列中是否所有值都相同\n",
    "    unique_values = data[col].unique()\n",
    "    if len(unique_values) == 1:\n",
    "        # 输出列名和该列唯一的数值\n",
    "        print(f\"列 '{col}' 的所有值相同，值为: {unique_values[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除指定的列: ['ground_heat', 'measured_albedo', 'surface_temperature(meas)', '24h_wind_drift', 'erosion_mass_loss', 'virtual_lysimeter_under_the_soil', 'temperature@0.25m', 'temperature@0.5m', 'temperature@1m', 'temperature@-0.25m', 'temperature@-0.1m', 'profile_type', 'z_S5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"ground_heat\", \"measured_albedo\", \"surface_temperature(meas)\", \n",
    "    \"24h_wind_drift\", \"erosion_mass_loss\", \"virtual_lysimeter_under_the_soil\", \n",
    "    \"temperature@0.25m\", \"temperature@0.5m\", \"temperature@1m\", \n",
    "    \"temperature@-0.25m\", \"temperature@-0.1m\", \"profile_type\", \"z_S5\"\n",
    "]\n",
    "\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "data.to_csv(file_path, index=False)\n",
    "print(\"已删除指定的列:\", columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多少行包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含缺失值的行数: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',', na_values=-999, low_memory=False)\n",
    "rows_with_missing_values = data.isnull().any(axis=1).sum()\n",
    "print(f\"包含缺失值的行数: {rows_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哪些列包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含缺失值的列数: 2\n",
      "包含缺失值的列名:\n",
      "ground_heat_at_soil_interface\n",
      "stability_index_5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',', na_values=-999, low_memory=False)\n",
    "columns_with_missing_values = data.columns[data.isnull().any()]\n",
    "print(f\"包含缺失值的列数: {len(columns_with_missing_values)}\")\n",
    "print(\"包含缺失值的列名:\")\n",
    "for col in columns_with_missing_values:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除包含缺失值的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除指定的列: ['ground_heat_at_soil_interface', 'stability_index_5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"ground_heat_at_soil_interface\",  \"stability_index_5\"\n",
    "]\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "data.to_csv(file_path, index=False)\n",
    "print(\"已删除指定的列:\", columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:1901,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',',low_memory=False)\n",
    "x,y = data.shape\n",
    "print(f'行数:{x},列数:{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化后的数据已保存到 /home/develop/Station/data/csv/station_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = data.copy()\n",
    "if not data_normalized.select_dtypes(include=['number']).empty:\n",
    "    numeric_columns = data_normalized.select_dtypes(include=['number']).columns\n",
    "    data_normalized[numeric_columns] = scaler.fit_transform(data_normalized[numeric_columns])\n",
    "output_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data_normalized.to_csv(output_path, index=False)\n",
    "print(f\"归一化后的数据已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:1901,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',',low_memory=False)\n",
    "x,y = data.shape\n",
    "print(f'行数:{x},列数:{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异常数据：(53, 128, 190, 343, 526, 612, 1321, 1652, 1823, 1932, 2024, 2135, 2352, 2683, 2794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用训练集训练训练GATv2模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.059586\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.052277\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114787\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114818\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.075661\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114789\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114787\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114787\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.054126\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114787\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114786\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114787\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.072734\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114783\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114787\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114787\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.114787\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.096854\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.092163\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114788\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.055961\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114788\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114786\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.110815\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.047062\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.098422\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114637\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114788\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.114781\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114785\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.055257\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114787\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.063658\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.114788\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.114787\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.114798\n",
      "\n",
      "Best Hyperparameters: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Best Loss: 0.04706227779388428\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "file_path = '/home/develop/GATv2-VAE_NewData/data/csv/ceshi_temp_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "features = torch.tensor(data.values, dtype=torch.float32)  \n",
    "features = features.T  \n",
    "num_features = features.size(0)\n",
    "adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "graph_data = Data(x=features, edge_index=edge_index)\n",
    "\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_channels': [8, 16, 32],\n",
    "    'heads': [1, 2, 4],\n",
    "    'learning_rate': [0.001, 0.005],\n",
    "    'weight_decay': [0.0, 1e-4]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "log_dir = \"/home/develop/GATv2-VAE_NewData/Result\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"GATv2_Train.log\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    for params in grid:\n",
    "        log.write(f\"Training with params: {params}\\n\")\n",
    "        print(f\"Training with params: {params}\")\n",
    "    \n",
    "        model = GATv2Net(\n",
    "            in_channels=features.size(1),\n",
    "            hidden_channels=params['hidden_channels'],\n",
    "            out_channels=features.size(1),\n",
    "            heads=params['heads']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        graph_data = graph_data.to(device)\n",
    "        model.train()\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        for epoch in range(100):  # 最大训练轮数\n",
    "            optimizer.zero_grad()\n",
    "            out = model(graph_data.x, graph_data.edge_index)  # 前向传播\n",
    "            loss = loss_fn(out, graph_data.x)  # 重构损失\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 参数更新\n",
    "\n",
    "            early_stopping.step(loss.item())\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        final_loss = early_stopping.best_loss\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "        log.write(f\"Final Loss for params {params}: {final_loss:.6f}\\n\")\n",
    "        log.write(\"-\" * 50 + \"\\n\")\n",
    "        print(f\"Final Loss for params {params}: {final_loss:.6f}\")\n",
    "\n",
    "best_result = min(results, key=lambda x: x['final_loss'])\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(\"\\nBest Hyperparameters:\\n\")\n",
    "    log.write(str(best_result['params']) + \"\\n\")\n",
    "    log.write(f\"Best Loss: {best_result['final_loss']:.6f}\\n\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_result['params'])\n",
    "print(\"Best Loss:\", best_result['final_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters:\n",
    "\n",
    "{'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
    "\n",
    "**Best Loss: 0.047062**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据确定超参数训练并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 超参数\n",
    "hidden_channels = 8  # 隐藏层维度\n",
    "heads = 4  # 多头注意力\n",
    "learning_rate = 0.001  # 学习率\n",
    "weight_decay = 0.0  # 权重衰减\n",
    "epochs = 100  # 最大训练轮数\n",
    "patience = 10  # 早停容忍次数\n",
    "save_path = \"/home/develop/GATv2-VAE_NewData/Model/GATv2_trained.pth\"  \n",
    "log_file = \"/home/develop/GATv2-VAE_NewData/Result/GATv2_Train.log\"  \n",
    "\n",
    "file_path = '/home/develop/GATv2-VAE_NewData/data/csv/ceshi_temp_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "features = torch.tensor(data.values, dtype=torch.float32) \n",
    "\n",
    "features = features.T\n",
    "num_features = features.size(0)\n",
    "adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "\n",
    "graph_data = Data(x=features, edge_index=edge_index)\n",
    "\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "device = torch.device('cpu')  # 强制使用 CPU\n",
    "model = GATv2Net(\n",
    "    in_channels=features.size(1),\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=features.size(1),  # 输出维度等于输入维度（重构任务）\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "model.train()\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data.x, graph_data.edge_index)  # 前向传播\n",
    "        loss = loss_fn(out, graph_data.x)  # 重构损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 参数更新\n",
    "\n",
    "        log.write(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\\n\")\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "        early_stopping.step(loss.item())\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            log.write(f\"Early stopping triggered at epoch {epoch}\\n\")\n",
    "            break\n",
    "torch.save(model, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(f\"Model saved to {save_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GATv2进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "file_path = '/home/develop/GATv2-VAE_NewData/data/csv/ceshi_temp_normalized.csv'\n",
    "data_new = pd.read_csv(file_path)\n",
    "\n",
    "features_new = torch.tensor(data_new.values, dtype=torch.float32).T \n",
    "num_features_new = features_new.size(0)\n",
    "\n",
    "adj_matrix_new = torch.ones((num_features_new, num_features_new)) - torch.eye(num_features_new)  # 完全图\n",
    "edge_index_new = dense_to_sparse(adj_matrix_new)[0]\n",
    "\n",
    "device = torch.device('cpu')\n",
    "graph_data_new = Data(x=features_new, edge_index=edge_index_new)\n",
    "\n",
    "model = torch.load(\"/home/develop/GATv2-VAE_NewData/Model/GATv2_trained.pth\")\n",
    "model.eval() \n",
    "graph_data_new = graph_data_new.to(device)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    output_new = model(graph_data_new.x, graph_data_new.edge_index)\n",
    "\n",
    "print(\"GATv2 模型的输出：\")\n",
    "print(output_new)\n",
    "print(f\"模型输出的维度: {output_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_new_transposed = output_new.T \n",
    "output_new_transposed_np = output_new_transposed.detach().numpy()\n",
    "print(output_new_transposed.shape)\n",
    "print(type(output_new_transposed))\n",
    "output_new_transposed_df = pd.DataFrame(output_new_transposed.numpy())\n",
    "\n",
    "print(output_new_transposed_df.shape)\n",
    "print(type(output_new_transposed_df))\n",
    "print(output_new_transposed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from datetime import datetime\n",
    "\n",
    "X = output_new_transposed_df.values  \n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# 采样层\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling_layer = layers.Lambda(sampling)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_1 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, original_dim, latent_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(original_dim=original_dim)\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "            reconstruction_loss = tf.reduce_mean(reconstruction_loss_fn(data, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1\n",
    "            )\n",
    "            total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        reconstruction_loss = tf.reduce_mean(reconstruction_loss_fn(data, reconstruction))\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1\n",
    "        )\n",
    "        total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "latent_dims = [5, 10, 15, 20]  \n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs_list = [20, 30, 50]\n",
    "log_file = \"/home/develop/GATv2-VAE_NewData/Result/GATv2VAE.log\"\n",
    "save_dir = \"/home/develop/GATv2-VAE_NewData/Result/pic/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"Training Log\\n\")\n",
    "    log.write(\"Parameters: latent_dim, batch_size, epochs\\n\")\n",
    "    log.write(\"Results: reconstruction_error_threshold, anomalies_detected, training_time\\n\")\n",
    "    log.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                input_dim = X_train.shape[1]\n",
    "\n",
    "                vae = VAE(original_dim=input_dim, latent_dim=latent_dim)\n",
    "                vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "\n",
    "                start_time = datetime.now()\n",
    "                history = vae.fit(\n",
    "                    X_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test,),  # 只传递验证数据，不包括目标\n",
    "                    verbose=0\n",
    "                )\n",
    "                training_time = datetime.now() - start_time\n",
    "\n",
    "                # 绘制训练损失\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                if 'val_loss' in history.history:\n",
    "                    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "                    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "                else:\n",
    "                    plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
    "                \n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.legend()\n",
    "                plt.title(f\"Loss (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}loss_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "\n",
    "                X_pred = vae.predict(X_test, verbose=0)\n",
    "                reconstruction_error = np.mean(np.square(X_test - X_pred), axis=1)\n",
    "                threshold = np.percentile(reconstruction_error, 95)\n",
    "                anomalies = reconstruction_error > threshold\n",
    "                plt.figure()\n",
    "                plt.hist(reconstruction_error, bins=50)\n",
    "                plt.xlabel(\"Reconstruction Error\")\n",
    "                plt.ylabel(\"Number of Samples\")\n",
    "                plt.title(f\"Error Dist. (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}error_dist_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "\n",
    "                log.write(f\"latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\\n\")\n",
    "                log.write(f\"reconstruction_error_threshold={threshold:.4f}, anomalies_detected={np.sum(anomalies)}, training_time={training_time}\\n\")\n",
    "                log.write(\"-\" * 80 + \"\\n\")\n",
    "                print(f\"Params: latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                print(f\"Reconstruction Error Threshold: {threshold:.4f}\")\n",
    "                print(f\"Anomalies detected: {np.sum(anomalies)}\")\n",
    "                print(f\"Training Time: {training_time}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机将285条正常数据添加到测试集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ceshi_file = '/home/develop/GATv2-VAE_NewData/data/csv/ceshi_temp_normalized.csv'\n",
    "test_file = '/home/develop/GATv2-VAE_NewData/data/csv/test_temp.csv'\n",
    "ceshi_data = pd.read_csv(ceshi_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "selected_data = ceshi_data.sample(n=285, random_state=42).copy()\n",
    "selected_data['yichang'] = 0\n",
    "test_data = pd.concat([test_data, selected_data], ignore_index=True)\n",
    "# 随机打乱\n",
    "test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# 保存回 test_temp.csv\n",
    "test_data.to_csv(test_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    if 'yichang' in data.columns:\n",
    "        features = data.drop(columns=['yichang']).values\n",
    "    else:\n",
    "        features = data.values\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32).T \n",
    "    num_features = features.size(0)\n",
    "    \n",
    "    adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "    edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "    \n",
    "    return Data(x=features, edge_index=edge_index)\n",
    "\n",
    "def inference(model, file_path):\n",
    "    model.eval()\n",
    "    graph_data_new = prepare_data(file_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_new = model(graph_data_new.x, graph_data_new.edge_index)  # 获得模型输出\n",
    "\n",
    "    print(\"GATv2 模型的输出：\")\n",
    "    print(output_new) \n",
    "    print(f\"模型输出的维度: {output_new.shape}\")\n",
    "    return output_new.T.numpy()\n",
    "\n",
    "# VAE相关部分\n",
    "def build_vae(input_dim, latent_dim):\n",
    "    class Encoder(layers.Layer):\n",
    "        def __init__(self, latent_dim, **kwargs):\n",
    "            super(Encoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(42, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(84, activation=\"relu\")\n",
    "            self.dense_mean = layers.Dense(latent_dim)\n",
    "            self.dense_log_var = layers.Dense(latent_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            z_mean = self.dense_mean(x)\n",
    "            z_log_var = self.dense_log_var(x)\n",
    "            return z_mean, z_log_var\n",
    "\n",
    "    class Decoder(layers.Layer):\n",
    "        def __init__(self, original_dim, **kwargs):\n",
    "            super(Decoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(84, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(42, activation=\"relu\")\n",
    "            self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            return self.dense_output(x)\n",
    "\n",
    "    class VAE(Model):\n",
    "        def __init__(self, original_dim, latent_dim, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.original_dim = original_dim\n",
    "            self.encoder = Encoder(latent_dim=latent_dim)\n",
    "            self.decoder = Decoder(original_dim=original_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = self.encoder(inputs)\n",
    "            epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "            z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "            return self.decoder(z)\n",
    "\n",
    "    vae = VAE(original_dim=input_dim, latent_dim=latent_dim)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "    return vae\n",
    "\n",
    "hidden_channels = 8  # 隐藏层维度\n",
    "heads = 4  # 多头注意力\n",
    "\n",
    "# 数据路径\n",
    "data_file_path = '/home/develop/GATv2-VAE_NewData/data/csv/test_temp.csv'\n",
    "device = torch.device('cpu')\n",
    "graph_data = prepare_data(data_file_path)\n",
    "in_channels = graph_data.x.size(1)\n",
    "out_channels = graph_data.x.size(1)  \n",
    "\n",
    "model = GATv2Net(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "output_new_transposed = inference(model, data_file_path)\n",
    "output_new_transposed_df = pd.DataFrame(output_new_transposed)\n",
    "latent_dim = 20\n",
    "input_dim = output_new_transposed_df.shape[1]\n",
    "\n",
    "vae = build_vae(input_dim, latent_dim)\n",
    "\n",
    "X_pred = vae.predict(output_new_transposed_df.values, batch_size=16, verbose=0)\n",
    "reconstruction_error = np.mean(np.square(output_new_transposed_df.values - X_pred), axis=1)\n",
    "\n",
    "threshold = np.percentile(reconstruction_error, 95)\n",
    "anomalies = reconstruction_error > threshold\n",
    "\n",
    "anomalous_indices = np.where(anomalies)[0]\n",
    "print(\"Anomalous indices:\")\n",
    "print(anomalous_indices)\n",
    "# 数据路径\n",
    "data_file_path = '/home/develop/GATv2-VAE_NewData/data/csv/test_temp.csv'\n",
    "\n",
    "data = pd.read_csv(data_file_path)\n",
    "if 'yichang' not in data.columns:\n",
    "    print(\"No yichang column found.\")\n",
    "else:\n",
    "    # 获取指定索引对应的 'yichang' 值\n",
    "    anomalous_origin_indices = data.loc[anomalous_indices, 'yichang'].values\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"Origin indices for the specified rows:\")\n",
    "    print(anomalous_origin_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

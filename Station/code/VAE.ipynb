{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smet文件转换为csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_folder = '/home/develop/Station/data/origin'\n",
    "output_folder = '/home/develop/Station/data/csvwithHeader/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "smet_files = [f for f in os.listdir(input_folder) if f.endswith('.smet')]\n",
    "\n",
    "for smet_file in smet_files:\n",
    "    smet_file_path = os.path.join(input_folder, smet_file)\n",
    "    data = pd.read_csv(smet_file_path, sep='\\t', header=None)\n",
    "    output_csv_path = os.path.join(output_folder, smet_file.replace('.smet', '.csv'))\n",
    "    data.to_csv(output_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除头部Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理并保存文件: station.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_dir = '/home/develop/Station/data/csvwithHeader/'\n",
    "output_dir = '/home/develop/Station/data/csv/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        input_filepath = os.path.join(input_dir, filename)     \n",
    "        with open(input_filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        start_index = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if '[DATA]' in line:\n",
    "                start_index = i + 1  \n",
    "                break\n",
    "\n",
    "        \n",
    "        if start_index is not None:\n",
    "            data_to_keep = lines[start_index:]\n",
    "            output_filepath = os.path.join(output_dir, filename)\n",
    "            with open(output_filepath, 'w') as output_file:\n",
    "                output_file.writelines(data_to_keep)\n",
    "            print(f\"处理并保存文件: {filename}\")\n",
    "        else:\n",
    "            print(f\"文件 {filename} 中没有找到 [DATA] 标签\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同列之间用逗号隔开同时将内容转换为数值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 station.csv 已成功转换，并保留了时间戳列。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_340351/2389777297.py:13: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定CSV文件夹路径\n",
    "folder_path = '/home/develop/Station/data/csv/'\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        data = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
    "        \n",
    "        # 假设时间戳是第一列（即索引0），我们将其分离出来\n",
    "        timestamps = data.iloc[:, 0]\n",
    "        \n",
    "        # 对剩余的列进行数值转换，无法转换的内容置为NaN\n",
    "        numeric_data = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # 将时间戳和数值数据重新合并\n",
    "        processed_data = pd.concat([timestamps, numeric_data], axis=1)\n",
    "        \n",
    "        # 将DataFrame保存为新的CSV文件，使用逗号作为分隔符\n",
    "        processed_data.to_csv(file_path, index=False, header=False, sep=',')\n",
    "        \n",
    "        print(f\"文件 {filename} 已成功转换，并保留了时间戳列。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列数匹配，表头将被添加到文件中: station.csv\n",
      "表头已成功添加到 station.csv。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = '/home/develop/Station/data/csv/'\n",
    "\n",
    "# 定义表头（列名）\n",
    "header = [\n",
    "    \"timestamp\",\"sensible_heat\", \"latent_heat\", \"ground_heat\", \"ground_temperature\", \n",
    "    \"ground_heat_at_soil_interface\", \"rain_energy\", \"outgoing_long_wave_radiation\", \n",
    "    \"incoming_long_wave_radiation\", \"net_long_wave_radiation\", \"reflected_short_wave_radiation\", \n",
    "    \"incoming_short_wave_radiation\", \"net_short_wave_radiation\", \"parametrized_albedo\", \n",
    "    \"measured_albedo\", \"incoming_short_wave_on_horizontal\", \"direct_incoming_short_wave\", \n",
    "    \"diffuse_incoming_short_wave\", \"air_temperature\", \"surface_temperature(mod)\", \n",
    "    \"surface_temperature(meas)\", \"bottom_temperature\", \"relative_humidity\", \"wind_velocity\", \n",
    "    \"wind_velocity_drift\", \"wind_direction\", \"solid_precipitation_rate\", \"snow_height(mod)\", \n",
    "    \"snow_height(meas)\", \"hoar_size\", \"24h_wind_drift\", \"24h_height_of_new_snow\", \n",
    "    \"3d_sum_of_daily_height_of_new_snow\", \"snow_water_equivalent\", \"total_amount_of_water\", \n",
    "    \"erosion_mass_loss\", \"rain_rate\", \"virtual_lysimeter\", \n",
    "    \"virtual_lysimeter_under_the_soil\", \"sublimation_mass\", \"evaporated_mass\", \n",
    "    \"temperature@0.25m\", \"temperature@0.5m\", \"temperature@1m\", \"temperature@-0.25m\", \n",
    "    \"temperature@-0.1m\", \"profile_type\", \"stability_class\", \"z_Sdef\", \n",
    "    \"deformation_rate_stability_index\", \"z_Sn38\", \"natural_stability_index\", \"z_Sk38\", \n",
    "    \"Sk38_skier_stability_index\", \"z_SSI\", \"structural_stability_index\", \"z_S5\", \n",
    "    \"stability_index_5\"\n",
    "]\n",
    "\n",
    "\n",
    "# 遍历文件夹中的所有 CSV 文件\n",
    "for file_name in os.listdir(folder_path):    \n",
    "    # 拼接完整路径\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # 检查文件是否是 CSV 文件\n",
    "    if not file_path.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 读取 CSV 文件，以逗号为分隔符\n",
    "        data = pd.read_csv(file_path, header=None, sep=',')\n",
    "        \n",
    "        # 检查列数是否与表头匹配\n",
    "        if len(data.columns) == len(header):\n",
    "            print(f\"列数匹配，表头将被添加到文件中: {file_name}\")\n",
    "            # 添加表头\n",
    "            data.columns = header\n",
    "\n",
    "            # 将修改后的数据保存回 CSV 文件\n",
    "            data.to_csv(file_path, index=False, sep=',')\n",
    "            print(f\"表头已成功添加到 {file_name}。\")\n",
    "        else:\n",
    "            print(f\"列数不匹配！文件: {file_name}，实际列数为 {len(data.columns)}，表头参数个数为 {len(header)}。\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {file_name} 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历文件，找到值不变的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列 'ground_heat' 的所有值相同，值为: -999\n",
      "列 'measured_albedo' 的所有值相同，值为: -999\n",
      "列 'surface_temperature(meas)' 的所有值相同，值为: -999\n",
      "列 '24h_wind_drift' 的所有值相同，值为: 0.0\n",
      "列 'virtual_lysimeter_under_the_soil' 的所有值相同，值为: -999\n",
      "列 'temperature@0.25m' 的所有值相同，值为: -999\n",
      "列 'temperature@0.5m' 的所有值相同，值为: -999\n",
      "列 'temperature@1m' 的所有值相同，值为: -999\n",
      "列 'temperature@-0.25m' 的所有值相同，值为: -999\n",
      "列 'temperature@-0.1m' 的所有值相同，值为: -999\n",
      "列 'profile_type' 的所有值相同，值为: -1.0\n",
      "列 'z_S5' 的所有值相同，值为: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "# 遍历每一列\n",
    "for col in data.columns:\n",
    "    # 检查列中是否所有值都相同\n",
    "    unique_values = data[col].unique()\n",
    "    if len(unique_values) == 1:\n",
    "        # 输出列名和该列唯一的数值\n",
    "        print(f\"列 '{col}' 的所有值相同，值为: {unique_values[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除指定的列: ['ground_heat', 'measured_albedo', 'surface_temperature(meas)', '24h_wind_drift', 'erosion_mass_loss', 'virtual_lysimeter_under_the_soil', 'temperature@0.25m', 'temperature@0.5m', 'temperature@1m', 'temperature@-0.25m', 'temperature@-0.1m', 'profile_type', 'z_S5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "# 定义要删除的列名列表\n",
    "columns_to_drop = [\n",
    "    \"ground_heat\", \"measured_albedo\", \"surface_temperature(meas)\", \n",
    "    \"24h_wind_drift\", \"erosion_mass_loss\", \"virtual_lysimeter_under_the_soil\", \n",
    "    \"temperature@0.25m\", \"temperature@0.5m\", \"temperature@1m\", \n",
    "    \"temperature@-0.25m\", \"temperature@-0.1m\", \"profile_type\", \"z_S5\"\n",
    "]\n",
    "\n",
    "# 删除指定列\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# 保存修改后的数据回到 CSV 文件\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"已删除指定的列:\", columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多少行包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含缺失值的行数: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',', na_values=-999, low_memory=False)\n",
    "rows_with_missing_values = data.isnull().any(axis=1).sum()\n",
    "print(f\"包含缺失值的行数: {rows_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哪些列包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含缺失值的列数: 2\n",
      "包含缺失值的列名:\n",
      "ground_heat_at_soil_interface\n",
      "stability_index_5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, header=0, sep=',', na_values=-999, low_memory=False)\n",
    "\n",
    "columns_with_missing_values = data.columns[data.isnull().any()]\n",
    "\n",
    "print(f\"包含缺失值的列数: {len(columns_with_missing_values)}\")\n",
    "print(\"包含缺失值的列名:\")\n",
    "for col in columns_with_missing_values:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除包含缺失值的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除指定的列: ['ground_heat_at_soil_interface', 'stability_index_5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data = pd.read_csv(file_path, header=0, sep=',', low_memory=False)\n",
    "\n",
    "# 定义要删除的列名列表\n",
    "columns_to_drop = [\n",
    "    \"ground_heat_at_soil_interface\",  \"stability_index_5\"\n",
    "]\n",
    "\n",
    "# 删除指定列\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# 保存修改后的数据回到 CSV 文件\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"已删除指定的列:\", columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出文件形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:1901,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',',low_memory=False)\n",
    "x,y = data.shape\n",
    "print(f'行数:{x},列数:{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化后的数据已保存到 /home/develop/Station/data/csv/station_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 读取原始数据\n",
    "file_path = '/home/develop/Station/data/csv/station.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 初始化 MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 对数据进行归一化，忽略非数值列（如果有的话）\n",
    "# 创建一个副本，避免修改原始数据\n",
    "data_normalized = data.copy()\n",
    "\n",
    "# 假设第一列为时间戳，我们从第二列开始进行归一化\n",
    "# 如果有其他非数值列，需要进一步调整以下代码来排除这些列\n",
    "if not data_normalized.select_dtypes(include=['number']).empty:\n",
    "    numeric_columns = data_normalized.select_dtypes(include=['number']).columns\n",
    "    data_normalized[numeric_columns] = scaler.fit_transform(data_normalized[numeric_columns])\n",
    "\n",
    "# 保存归一化后的数据\n",
    "output_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data_normalized.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"归一化后的数据已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数:1901,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data = pd.read_csv(file_path, header=0, sep=',',low_memory=False)\n",
    "x,y = data.shape\n",
    "print(f'行数:{x},列数:{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将雪崩前后15h的数据添加到测试集，并从原文件中删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "test_file_path = '/home/develop/Station/data/csv/test_station.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "subset_df = df.iloc[1394:1425]\n",
    "subset_df.to_csv(test_file_path, index=False)\n",
    "df = df.drop(df.index[1394:1425])\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源文件行数:1870,列数:43\n",
      "测试集行数:31,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path1 = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "file_path2 = '/home/develop/Station/data/csv/test_station.csv'\n",
    "data1 = pd.read_csv(file_path1, header=0, sep=',',low_memory=False)\n",
    "data2 = pd.read_csv(file_path2, header=0, sep=',',low_memory=False)\n",
    "x1,y1 = data1.shape\n",
    "x2,y2 = data2.shape\n",
    "print(f'源文件行数:{x1},列数:{y1}')\n",
    "print(f'测试集行数:{x2},列数:{y2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从源文件中随机选取160条数据添加到测试集兵打乱顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 160 rows to /home/develop/Station/data/csv/test_station.csv and updated /home/develop/Station/data/csv/station_normalized.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取源文件\n",
    "source_file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data = pd.read_csv(source_file_path)\n",
    "\n",
    "# 从源文件中随机选取160条数据\n",
    "random_sample = data.sample(n=160, random_state=42)\n",
    "\n",
    "# 读取目标文件，如果文件不存在则创建一个空的 DataFrame\n",
    "target_file_path = '/home/develop/Station/data/csv/test_station.csv'\n",
    "try:\n",
    "    target_data = pd.read_csv(target_file_path)\n",
    "except FileNotFoundError:\n",
    "    target_data = pd.DataFrame()\n",
    "\n",
    "# 将选取的数据添加到目标文件\n",
    "target_data = pd.concat([target_data, random_sample], ignore_index=True)\n",
    "\n",
    "# 保存更新后的目标文件\n",
    "target_data.to_csv(target_file_path, index=False)\n",
    "\n",
    "# 删除源文件中选中的160条数据\n",
    "data = data.drop(random_sample.index)\n",
    "\n",
    "# 打散源文件顺序\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 保存更新后的源文件\n",
    "data.to_csv(source_file_path, index=False)\n",
    "\n",
    "print(f\"Successfully added 160 rows to {target_file_path} and updated {source_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源文件行数:1710,列数:43\n",
      "测试集行数:191,列数:43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path1 = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "file_path2 = '/home/develop/Station/data/csv/test_station.csv'\n",
    "data1 = pd.read_csv(file_path1, header=0, sep=',',low_memory=False)\n",
    "data2 = pd.read_csv(file_path2, header=0, sep=',',low_memory=False)\n",
    "x1,y1 = data1.shape\n",
    "x2,y2 = data2.shape\n",
    "print(f'源文件行数:{x1},列数:{y1}')\n",
    "print(f'测试集行数:{x2},列数:{y2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用训练集训练训练GATv2模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.054801\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.050240\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.124047\n",
      "Training with params: {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124141\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.047807\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.044654\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.124123\n",
      "Training with params: {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124130\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.045562\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.085876\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.061148\n",
      "Training with params: {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 1, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124143\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.050599\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.043106\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.074147\n",
      "Training with params: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.041614\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.042634\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.053695\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.074482\n",
      "Training with params: {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124129\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.047134\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.124109\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.124125\n",
      "Training with params: {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 2, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124127\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.043044\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.054039\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.108223\n",
      "Training with params: {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.068508\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.048164\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.046540\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.060158\n",
      "Training with params: {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 16, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.057709\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0}: 0.048337\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.001, 'weight_decay': 0.0001}: 0.050552\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0}: 0.124121\n",
      "Training with params: {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Final Loss for params {'heads': 4, 'hidden_channels': 32, 'learning_rate': 0.005, 'weight_decay': 0.0001}: 0.124126\n",
      "\n",
      "Best Hyperparameters: {'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
      "Best Loss: 0.04161439463496208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# 读取数据\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(columns=data.columns[0])\n",
    "features = torch.tensor(data.values, dtype=torch.float32)  \n",
    "\n",
    "# 转置特征矩阵，适配 GAT 输入 (节点数, 特征维度)\n",
    "features = features.T  # \n",
    "\n",
    "# 创建边索引（完全图假设，每个特征和其他特征都有边）\n",
    "num_features = features.size(0)\n",
    "adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "\n",
    "# 构造图数据\n",
    "graph_data = Data(x=features, edge_index=edge_index)\n",
    "\n",
    "# GATv2 模型定义\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 网格搜索的超参数组合\n",
    "param_grid = {\n",
    "    'hidden_channels': [8, 16, 32],\n",
    "    'heads': [1, 2, 4],\n",
    "    'learning_rate': [0.001, 0.005],\n",
    "    'weight_decay': [0.0, 1e-4]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# 早停策略\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 日志保存路径\n",
    "log_dir = \"/home/develop/Station/Result\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"GATv2_Train.log\")\n",
    "\n",
    "# 训练过程\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    for params in grid:\n",
    "        # 打印当前超参数组合\n",
    "        log.write(f\"Training with params: {params}\\n\")\n",
    "        print(f\"Training with params: {params}\")\n",
    "        \n",
    "        # 模型实例化\n",
    "        model = GATv2Net(\n",
    "            in_channels=features.size(1),\n",
    "            hidden_channels=params['hidden_channels'],\n",
    "            out_channels=features.size(1),\n",
    "            heads=params['heads']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        graph_data = graph_data.to(device)\n",
    "        model.train()\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        for epoch in range(100):  # 最大训练轮数\n",
    "            optimizer.zero_grad()\n",
    "            out = model(graph_data.x, graph_data.edge_index)  # 前向传播\n",
    "            loss = loss_fn(out, graph_data.x)  # 重构损失\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 参数更新\n",
    "\n",
    "            early_stopping.step(loss.item())\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        # 记录结果\n",
    "        final_loss = early_stopping.best_loss\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "        log.write(f\"Final Loss for params {params}: {final_loss:.6f}\\n\")\n",
    "        log.write(\"-\" * 50 + \"\\n\")\n",
    "        print(f\"Final Loss for params {params}: {final_loss:.6f}\")\n",
    "\n",
    "# 打印最佳超参数组合\n",
    "best_result = min(results, key=lambda x: x['final_loss'])\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(\"\\nBest Hyperparameters:\\n\")\n",
    "    log.write(str(best_result['params']) + \"\\n\")\n",
    "    log.write(f\"Best Loss: {best_result['final_loss']:.6f}\\n\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_result['params'])\n",
    "print(\"Best Loss:\", best_result['final_loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters:\n",
    "\n",
    "{'heads': 2, 'hidden_channels': 8, 'learning_rate': 0.005, 'weight_decay': 0.0001}\n",
    "\n",
    "**Best Loss: 0.04161439463496208**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据确定超参数训练并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 0.246137\n",
      "Epoch 1/100, Loss: 0.242335\n",
      "Epoch 2/100, Loss: 0.221502\n",
      "Epoch 3/100, Loss: 0.209130\n",
      "Epoch 4/100, Loss: 0.198045\n",
      "Epoch 5/100, Loss: 0.184760\n",
      "Epoch 6/100, Loss: 0.168721\n",
      "Epoch 7/100, Loss: 0.156590\n",
      "Epoch 8/100, Loss: 0.147974\n",
      "Epoch 9/100, Loss: 0.140580\n",
      "Epoch 10/100, Loss: 0.137008\n",
      "Epoch 11/100, Loss: 0.131616\n",
      "Epoch 12/100, Loss: 0.129241\n",
      "Epoch 13/100, Loss: 0.127896\n",
      "Epoch 14/100, Loss: 0.127150\n",
      "Epoch 15/100, Loss: 0.126830\n",
      "Epoch 16/100, Loss: 0.126769\n",
      "Epoch 17/100, Loss: 0.126708\n",
      "Epoch 18/100, Loss: 0.126368\n",
      "Epoch 19/100, Loss: 0.126004\n",
      "Epoch 20/100, Loss: 0.125727\n",
      "Epoch 21/100, Loss: 0.125502\n",
      "Epoch 22/100, Loss: 0.125308\n",
      "Epoch 23/100, Loss: 0.125096\n",
      "Epoch 24/100, Loss: 0.124886\n",
      "Epoch 25/100, Loss: 0.124762\n",
      "Epoch 26/100, Loss: 0.124687\n",
      "Epoch 27/100, Loss: 0.124630\n",
      "Epoch 28/100, Loss: 0.124571\n",
      "Epoch 29/100, Loss: 0.124538\n",
      "Epoch 30/100, Loss: 0.124498\n",
      "Epoch 31/100, Loss: 0.124465\n",
      "Epoch 32/100, Loss: 0.124443\n",
      "Epoch 33/100, Loss: 0.124427\n",
      "Epoch 34/100, Loss: 0.124410\n",
      "Epoch 35/100, Loss: 0.124389\n",
      "Epoch 36/100, Loss: 0.124365\n",
      "Epoch 37/100, Loss: 0.124342\n",
      "Epoch 38/100, Loss: 0.124321\n",
      "Epoch 39/100, Loss: 0.124306\n",
      "Epoch 40/100, Loss: 0.124288\n",
      "Epoch 41/100, Loss: 0.124269\n",
      "Epoch 42/100, Loss: 0.124252\n",
      "Epoch 43/100, Loss: 0.124235\n",
      "Epoch 44/100, Loss: 0.124223\n",
      "Epoch 45/100, Loss: 0.124213\n",
      "Epoch 46/100, Loss: 0.124204\n",
      "Epoch 47/100, Loss: 0.124196\n",
      "Epoch 48/100, Loss: 0.124189\n",
      "Epoch 49/100, Loss: 0.124183\n",
      "Epoch 50/100, Loss: 0.124177\n",
      "Epoch 51/100, Loss: 0.124172\n",
      "Epoch 52/100, Loss: 0.124168\n",
      "Epoch 53/100, Loss: 0.124165\n",
      "Epoch 54/100, Loss: 0.124163\n",
      "Epoch 55/100, Loss: 0.124161\n",
      "Epoch 56/100, Loss: 0.124160\n",
      "Epoch 57/100, Loss: 0.124168\n",
      "Epoch 58/100, Loss: 0.124162\n",
      "Epoch 59/100, Loss: 0.124157\n",
      "Epoch 60/100, Loss: 0.124156\n",
      "Epoch 61/100, Loss: 0.124152\n",
      "Epoch 62/100, Loss: 0.124150\n",
      "Epoch 63/100, Loss: 0.124147\n",
      "Epoch 64/100, Loss: 0.124144\n",
      "Epoch 65/100, Loss: 0.124144\n",
      "Epoch 66/100, Loss: 0.124143\n",
      "Epoch 67/100, Loss: 0.124141\n",
      "Epoch 68/100, Loss: 0.124140\n",
      "Epoch 69/100, Loss: 0.124140\n",
      "Epoch 70/100, Loss: 0.124139\n",
      "Epoch 71/100, Loss: 0.124138\n",
      "Epoch 72/100, Loss: 0.124138\n",
      "Epoch 73/100, Loss: 0.124137\n",
      "Epoch 74/100, Loss: 0.124136\n",
      "Epoch 75/100, Loss: 0.124136\n",
      "Epoch 76/100, Loss: 0.124136\n",
      "Epoch 77/100, Loss: 0.124136\n",
      "Epoch 78/100, Loss: 0.124145\n",
      "Epoch 79/100, Loss: 0.124135\n",
      "Epoch 80/100, Loss: 0.124387\n",
      "Epoch 81/100, Loss: 0.124309\n",
      "Epoch 82/100, Loss: 0.124294\n",
      "Epoch 83/100, Loss: 0.124171\n",
      "Epoch 84/100, Loss: 0.124250\n",
      "Epoch 85/100, Loss: 0.124252\n",
      "Epoch 86/100, Loss: 0.124160\n",
      "Epoch 87/100, Loss: 0.124172\n",
      "Epoch 88/100, Loss: 0.124198\n",
      "Epoch 89/100, Loss: 0.124195\n",
      "Early stopping triggered at epoch 89\n",
      "Model saved to /home/develop/Station/Model/GATv2_trained.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 超参数\n",
    "hidden_channels = 8  # 隐藏层维度\n",
    "heads = 2  # 多头注意力\n",
    "learning_rate = 0.005  # 学习率\n",
    "weight_decay = 0.0001  # 权重衰减\n",
    "epochs = 100  # 最大训练轮数\n",
    "patience = 10  # 早停容忍次数\n",
    "save_path = \"/home/develop/Station/Model/GATv2_trained.pth\"  # 保存路径\n",
    "log_file = \"/home/develop/Station/Result/GATv2_Train.log\"  # 日志路径\n",
    "\n",
    "# 数据加载与预处理\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.drop(columns=data.columns[0])\n",
    "features = torch.tensor(data.values, dtype=torch.float32) \n",
    "\n",
    "# 转置特征矩阵，适配 GAT 输入 (节点数, 特征维度)\n",
    "features = features.T\n",
    "\n",
    "# 创建边索引（完全图假设，每个特征和其他特征都有边）\n",
    "num_features = features.size(0)\n",
    "adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "\n",
    "# 构造图数据\n",
    "graph_data = Data(x=features, edge_index=edge_index)\n",
    "\n",
    "# GATv2 模型定义\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 早停机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 确保日志目录存在\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "# 初始化模型和优化器\n",
    "device = torch.device('cpu')  # 强制使用 CPU\n",
    "model = GATv2Net(\n",
    "    in_channels=features.size(1),\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=features.size(1),  # 输出维度等于输入维度（重构任务）\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 确保 graph_data 的所有元素移动到设备\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# 模型训练\n",
    "model.train()\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "with open(log_file, \"w\") as log:\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data.x, graph_data.edge_index)  # 前向传播\n",
    "        loss = loss_fn(out, graph_data.x)  # 重构损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 参数更新\n",
    "\n",
    "        # 记录日志\n",
    "        log.write(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\\n\")\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "        # 检查早停\n",
    "        early_stopping.step(loss.item())\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            log.write(f\"Early stopping triggered at epoch {epoch}\\n\")\n",
    "            break\n",
    "\n",
    "# 保存训练好的模型\n",
    "torch.save(model, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "with open(log_file, \"a\") as log:\n",
    "    log.write(f\"Model saved to {save_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GATv2进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATv2 模型的输出：\n",
      "tensor([[0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109],\n",
      "        [0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109],\n",
      "        [0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109],\n",
      "        ...,\n",
      "        [0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109],\n",
      "        [0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109],\n",
      "        [0.3034, 0.3667, 0.3797,  ..., 0.1778, 0.3351, 0.3109]])\n",
      "模型输出的维度: torch.Size([42, 1710])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_649945/658681173.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/home/develop/Station/Model/GATv2_trained.pth\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "file_path = '/home/develop/Station/data/csv/station_normalized.csv'\n",
    "data_new = pd.read_csv(file_path)\n",
    "data_new = data_new.drop(columns=data_new.columns[0])\n",
    "\n",
    "features_new = torch.tensor(data_new.values, dtype=torch.float32).T \n",
    "num_features_new = features_new.size(0)\n",
    "\n",
    "# 3. 创建边索引（完全图假设，每个特征和其他特征都有边）\n",
    "adj_matrix_new = torch.ones((num_features_new, num_features_new)) - torch.eye(num_features_new)  # 完全图\n",
    "edge_index_new = dense_to_sparse(adj_matrix_new)[0]\n",
    "\n",
    "# 4. 创建图数据对象\n",
    "device = torch.device('cpu')  # 使用 CPU\n",
    "graph_data_new = Data(x=features_new, edge_index=edge_index_new)\n",
    "\n",
    "# 5. 加载训练好的 GATv2 模型\n",
    "model = torch.load(\"/home/develop/Station/Model/GATv2_trained.pth\")\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 6. 确保数据移到正确的设备\n",
    "graph_data_new = graph_data_new.to(device)\n",
    "\n",
    "# 7. 推理\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    output_new = model(graph_data_new.x, graph_data_new.edge_index)  # 获得模型输出\n",
    "\n",
    "# 8. 打印输出\n",
    "print(\"GATv2 模型的输出：\")\n",
    "print(output_new)  # 打印推理结果\n",
    "\n",
    "# 9. 输出数据的维度，确认是否匹配预期\n",
    "print(f\"模型输出的维度: {output_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1710, 42])\n",
      "<class 'torch.Tensor'>\n",
      "(1710, 42)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "            0         1         2         3         4         5         6   \\\n",
      "0     0.303360  0.303360  0.303360  0.303360  0.303360  0.303360  0.303360   \n",
      "1     0.366682  0.366682  0.366682  0.366682  0.366682  0.366682  0.366682   \n",
      "2     0.379706  0.379706  0.379706  0.379706  0.379706  0.379706  0.379706   \n",
      "3     0.283430  0.283430  0.283430  0.283430  0.283430  0.283430  0.283430   \n",
      "4     0.439225  0.439225  0.439225  0.439225  0.439225  0.439225  0.439225   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1705  0.393685  0.393685  0.393685  0.393685  0.393685  0.393685  0.393685   \n",
      "1706  0.359825  0.359825  0.359825  0.359825  0.359825  0.359825  0.359825   \n",
      "1707  0.177849  0.177849  0.177849  0.177849  0.177849  0.177849  0.177849   \n",
      "1708  0.335140  0.335140  0.335140  0.335140  0.335140  0.335140  0.335140   \n",
      "1709  0.310869  0.310869  0.310869  0.310869  0.310869  0.310869  0.310869   \n",
      "\n",
      "            7         8         9   ...        32        33        34  \\\n",
      "0     0.303360  0.303360  0.303360  ...  0.303360  0.303360  0.303360   \n",
      "1     0.366682  0.366682  0.366682  ...  0.366682  0.366682  0.366682   \n",
      "2     0.379706  0.379706  0.379707  ...  0.379707  0.379706  0.379706   \n",
      "3     0.283430  0.283430  0.283430  ...  0.283430  0.283430  0.283430   \n",
      "4     0.439225  0.439225  0.439225  ...  0.439225  0.439225  0.439225   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1705  0.393685  0.393685  0.393685  ...  0.393685  0.393685  0.393685   \n",
      "1706  0.359825  0.359825  0.359825  ...  0.359825  0.359825  0.359825   \n",
      "1707  0.177849  0.177849  0.177849  ...  0.177849  0.177849  0.177849   \n",
      "1708  0.335140  0.335140  0.335140  ...  0.335140  0.335140  0.335140   \n",
      "1709  0.310869  0.310869  0.310869  ...  0.310869  0.310869  0.310869   \n",
      "\n",
      "            35        36        37        38        39        40        41  \n",
      "0     0.303360  0.303360  0.303360  0.303360  0.303360  0.303360  0.303360  \n",
      "1     0.366682  0.366683  0.366682  0.366682  0.366682  0.366682  0.366682  \n",
      "2     0.379707  0.379707  0.379706  0.379706  0.379706  0.379706  0.379707  \n",
      "3     0.283430  0.283430  0.283430  0.283430  0.283430  0.283430  0.283430  \n",
      "4     0.439225  0.439225  0.439225  0.439225  0.439225  0.439225  0.439225  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1705  0.393685  0.393685  0.393685  0.393685  0.393685  0.393685  0.393685  \n",
      "1706  0.359825  0.359825  0.359825  0.359825  0.359825  0.359825  0.359825  \n",
      "1707  0.177849  0.177849  0.177849  0.177849  0.177849  0.177849  0.177849  \n",
      "1708  0.335140  0.335140  0.335140  0.335140  0.335140  0.335140  0.335140  \n",
      "1709  0.310869  0.310869  0.310869  0.310869  0.310869  0.310869  0.310869  \n",
      "\n",
      "[1710 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "output_new_transposed = output_new.T  # 转置后变为 [1186, 42]\n",
    "output_new_transposed_np = output_new_transposed.detach().numpy()  # 转换为 NumPy 数组\n",
    "\n",
    "print(output_new_transposed.shape)\n",
    "print(type(output_new_transposed))\n",
    "output_new_transposed_df = pd.DataFrame(output_new_transposed.numpy())\n",
    "\n",
    "print(output_new_transposed_df.shape)\n",
    "print(type(output_new_transposed_df))\n",
    "print(output_new_transposed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 16:32:20.457867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-19 16:32:20.502608: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-19 16:32:20.503578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 16:32:21.276586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: latent_dim=5, batch_size=16, epochs=20\n",
      "Reconstruction Error Threshold: 0.0153\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.595765\n",
      "\n",
      "Params: latent_dim=5, batch_size=16, epochs=30\n",
      "Reconstruction Error Threshold: 0.0149\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.653367\n",
      "\n",
      "Params: latent_dim=5, batch_size=16, epochs=50\n",
      "Reconstruction Error Threshold: 0.0143\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:06.900695\n",
      "\n",
      "Params: latent_dim=5, batch_size=32, epochs=20\n",
      "Reconstruction Error Threshold: 0.0135\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.578437\n",
      "\n",
      "Params: latent_dim=5, batch_size=32, epochs=30\n",
      "Reconstruction Error Threshold: 0.0146\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.312582\n",
      "\n",
      "Params: latent_dim=5, batch_size=32, epochs=50\n",
      "Reconstruction Error Threshold: 0.0145\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.632964\n",
      "\n",
      "Params: latent_dim=5, batch_size=64, epochs=20\n",
      "Reconstruction Error Threshold: 0.0190\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.159168\n",
      "\n",
      "Params: latent_dim=5, batch_size=64, epochs=30\n",
      "Reconstruction Error Threshold: 0.0148\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.626043\n",
      "\n",
      "Params: latent_dim=5, batch_size=64, epochs=50\n",
      "Reconstruction Error Threshold: 0.0163\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.155649\n",
      "\n",
      "Params: latent_dim=10, batch_size=16, epochs=20\n",
      "Reconstruction Error Threshold: 0.0158\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.458931\n",
      "\n",
      "Params: latent_dim=10, batch_size=16, epochs=30\n",
      "Reconstruction Error Threshold: 0.0157\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.523713\n",
      "\n",
      "Params: latent_dim=10, batch_size=16, epochs=50\n",
      "Reconstruction Error Threshold: 0.0157\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:06.673324\n",
      "\n",
      "Params: latent_dim=10, batch_size=32, epochs=20\n",
      "Reconstruction Error Threshold: 0.0149\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.517488\n",
      "\n",
      "Params: latent_dim=10, batch_size=32, epochs=30\n",
      "Reconstruction Error Threshold: 0.0146\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.200486\n",
      "\n",
      "Params: latent_dim=10, batch_size=32, epochs=50\n",
      "Reconstruction Error Threshold: 0.0155\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.778244\n",
      "\n",
      "Params: latent_dim=10, batch_size=64, epochs=20\n",
      "Reconstruction Error Threshold: 0.0163\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.809937\n",
      "\n",
      "Params: latent_dim=10, batch_size=64, epochs=30\n",
      "Reconstruction Error Threshold: 0.0161\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.699083\n",
      "\n",
      "Params: latent_dim=10, batch_size=64, epochs=50\n",
      "Reconstruction Error Threshold: 0.0137\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.609434\n",
      "\n",
      "Params: latent_dim=15, batch_size=16, epochs=20\n",
      "Reconstruction Error Threshold: 0.0158\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.501686\n",
      "\n",
      "Params: latent_dim=15, batch_size=16, epochs=30\n",
      "Reconstruction Error Threshold: 0.0160\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.506645\n",
      "\n",
      "Params: latent_dim=15, batch_size=16, epochs=50\n",
      "Reconstruction Error Threshold: 0.0128\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:06.885893\n",
      "\n",
      "Params: latent_dim=15, batch_size=32, epochs=20\n",
      "Reconstruction Error Threshold: 0.0172\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.582118\n",
      "\n",
      "Params: latent_dim=15, batch_size=32, epochs=30\n",
      "Reconstruction Error Threshold: 0.0153\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.267844\n",
      "\n",
      "Params: latent_dim=15, batch_size=32, epochs=50\n",
      "Reconstruction Error Threshold: 0.0140\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.641624\n",
      "\n",
      "Params: latent_dim=15, batch_size=64, epochs=20\n",
      "Reconstruction Error Threshold: 0.0166\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.200019\n",
      "\n",
      "Params: latent_dim=15, batch_size=64, epochs=30\n",
      "Reconstruction Error Threshold: 0.0178\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.637991\n",
      "\n",
      "Params: latent_dim=15, batch_size=64, epochs=50\n",
      "Reconstruction Error Threshold: 0.0137\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.555679\n",
      "\n",
      "Params: latent_dim=20, batch_size=16, epochs=20\n",
      "Reconstruction Error Threshold: 0.0119\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.434041\n",
      "\n",
      "Params: latent_dim=20, batch_size=16, epochs=30\n",
      "Reconstruction Error Threshold: 0.0141\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:04.465366\n",
      "\n",
      "Params: latent_dim=20, batch_size=16, epochs=50\n",
      "Reconstruction Error Threshold: 0.0145\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:06.851240\n",
      "\n",
      "Params: latent_dim=20, batch_size=32, epochs=20\n",
      "Reconstruction Error Threshold: 0.0143\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.583825\n",
      "\n",
      "Params: latent_dim=20, batch_size=32, epochs=30\n",
      "Reconstruction Error Threshold: 0.0132\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.310822\n",
      "\n",
      "Params: latent_dim=20, batch_size=32, epochs=50\n",
      "Reconstruction Error Threshold: 0.0130\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:05.673266\n",
      "\n",
      "Params: latent_dim=20, batch_size=64, epochs=20\n",
      "Reconstruction Error Threshold: 0.0135\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.199162\n",
      "\n",
      "Params: latent_dim=20, batch_size=64, epochs=30\n",
      "Reconstruction Error Threshold: 0.0150\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:02.681046\n",
      "\n",
      "Params: latent_dim=20, batch_size=64, epochs=50\n",
      "Reconstruction Error Threshold: 0.0136\n",
      "Anomalies detected: 18\n",
      "Training Time: 0:00:03.722218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from datetime import datetime\n",
    "\n",
    "# 假设output_new_transposed_df是GATv2模型的输出DataFrame\n",
    "# output_new_transposed_df = your_data_here\n",
    "\n",
    "# 1. 数据预处理\n",
    "X = output_new_transposed_df.values  # 转为numpy数组\n",
    "\n",
    "# 划分训练集和验证集，按8:2划分\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# 定义采样层\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# 定义编码器部分为独立的模型\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling_layer = layers.Lambda(sampling)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "# 定义解码器部分为独立的模型\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_1 = layers.Dense(32, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "# 定义VAE模型\n",
    "class VAE(Model):\n",
    "    def __init__(self, original_dim, latent_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(original_dim=original_dim)\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "            reconstruction_loss = tf.reduce_mean(reconstruction_loss_fn(data, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1\n",
    "            )\n",
    "            total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        reconstruction_loss = tf.reduce_mean(reconstruction_loss_fn(data, reconstruction))\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1\n",
    "        )\n",
    "        total_loss = reconstruction_loss + 0.1 * kl_loss\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# 设置超参数网格\n",
    "latent_dims = [5, 10, 15, 20]  # 潜在空间维度\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs_list = [20, 30, 50]\n",
    "log_file = \"/home/develop/Station/Result/GATv2VAE.log\"\n",
    "save_dir = \"/home/develop/Station/Result/pic/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 网格搜索训练过程\n",
    "with open(log_file, \"w\") as log:\n",
    "    log.write(\"Training Log\\n\")\n",
    "    log.write(\"Parameters: latent_dim, batch_size, epochs\\n\")\n",
    "    log.write(\"Results: reconstruction_error_threshold, anomalies_detected, training_time\\n\")\n",
    "    log.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                input_dim = X_train.shape[1]\n",
    "\n",
    "                # 构建VAE模型\n",
    "                vae = VAE(original_dim=input_dim, latent_dim=latent_dim)\n",
    "                vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "\n",
    "                # 训练模型\n",
    "                start_time = datetime.now()\n",
    "                history = vae.fit(\n",
    "                    X_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test,),  # 只传递验证数据，不包括目标\n",
    "                    verbose=0\n",
    "                )\n",
    "                training_time = datetime.now() - start_time\n",
    "\n",
    "                # 绘制训练损失\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                # 检查是否存在验证损失键\n",
    "                if 'val_loss' in history.history:\n",
    "                    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "                    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "                else:\n",
    "                    plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
    "                \n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.legend()\n",
    "                plt.title(f\"Loss (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}loss_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "\n",
    "                # 异常检测\n",
    "                X_pred = vae.predict(X_test, verbose=0)\n",
    "                reconstruction_error = np.mean(np.square(X_test - X_pred), axis=1)\n",
    "                threshold = np.percentile(reconstruction_error, 95)\n",
    "                anomalies = reconstruction_error > threshold\n",
    "\n",
    "                # 绘制重构误差分布\n",
    "                plt.figure()\n",
    "                plt.hist(reconstruction_error, bins=50)\n",
    "                plt.xlabel(\"Reconstruction Error\")\n",
    "                plt.ylabel(\"Number of Samples\")\n",
    "                plt.title(f\"Error Dist. (latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs})\")\n",
    "                plt.savefig(f\"{save_dir}error_dist_latent{latent_dim}_batch{batch_size}_epochs{epochs}.png\")\n",
    "                plt.close()\n",
    "\n",
    "                # 记录日志\n",
    "                log.write(f\"latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\\n\")\n",
    "                log.write(f\"reconstruction_error_threshold={threshold:.4f}, anomalies_detected={np.sum(anomalies)}, training_time={training_time}\\n\")\n",
    "                log.write(\"-\" * 80 + \"\\n\")\n",
    "                print(f\"Params: latent_dim={latent_dim}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                print(f\"Reconstruction Error Threshold: {threshold:.4f}\")\n",
    "                print(f\"Anomalies detected: {np.sum(anomalies)}\")\n",
    "                print(f\"Training Time: {training_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 anomalous indices and their 'timestamp' values:\n",
      "Index: 0, timestamp: 2024-02-04T02:00:00\n",
      "Index: 16, timestamp: 2024-02-04T18:00:00\n",
      "Index: 17, timestamp: 2024-02-04T19:00:00\n",
      "Index: 61, timestamp: 2024-01-08T20:00:00\n",
      "Index: 100, timestamp: 2024-01-30T17:00:00\n",
      "Index: 134, timestamp: 2024-02-20T06:00:00\n",
      "Index: 156, timestamp: 2023-12-20T19:00:00\n",
      "Index: 172, timestamp: 2023-12-20T01:00:00\n",
      "Index: 37, timestamp: 2023-12-18T07:00:00\n",
      "Index: 68, timestamp: 2024-02-24T19:00:00\n",
      "Index: 54, timestamp: 2024-01-31T05:00:00\n",
      "Index: 102, timestamp: 2024-01-25T08:00:00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from collections import Counter\n",
    "\n",
    "# GATv2 模型定义\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 移除 timestamp 列，如果存在的话\n",
    "    if 'timestamp' in data.columns:\n",
    "        features = data.drop(columns=['timestamp']).values\n",
    "    else:\n",
    "        features = data.values\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32).T \n",
    "    num_features = features.size(0)\n",
    "    \n",
    "    adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "    edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "    \n",
    "    return Data(x=features, edge_index=edge_index)\n",
    "\n",
    "def inference(model, file_path):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    graph_data_new = prepare_data(file_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_new = model(graph_data_new.x, graph_data_new.edge_index)  # 获得模型输出\n",
    "\n",
    "    return output_new.T.numpy()  # 返回转置后的numpy数组\n",
    "\n",
    "# VAE相关部分\n",
    "def build_vae(input_dim, latent_dim):\n",
    "    class Encoder(layers.Layer):\n",
    "        def __init__(self, latent_dim, **kwargs):\n",
    "            super(Encoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(64, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(32, activation=\"relu\")\n",
    "            self.dense_mean = layers.Dense(latent_dim)\n",
    "            self.dense_log_var = layers.Dense(latent_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            z_mean = self.dense_mean(x)\n",
    "            z_log_var = self.dense_log_var(x)\n",
    "            return z_mean, z_log_var\n",
    "\n",
    "    class Decoder(layers.Layer):\n",
    "        def __init__(self, original_dim, **kwargs):\n",
    "            super(Decoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(32, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(64, activation=\"relu\")\n",
    "            self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            return self.dense_output(x)\n",
    "\n",
    "    class VAE(Model):\n",
    "        def __init__(self, original_dim, latent_dim, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.original_dim = original_dim\n",
    "            self.encoder = Encoder(latent_dim=latent_dim)\n",
    "            self.decoder = Decoder(original_dim=original_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = self.encoder(inputs)\n",
    "            epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "            z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "            return self.decoder(z)\n",
    "\n",
    "    vae = VAE(original_dim=input_dim, latent_dim=latent_dim)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "    return vae\n",
    "\n",
    "def run_inference_multiple_times(model, vae, data_file_path, iterations=75):\n",
    "    anomaly_counts = Counter()\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        output_new_transposed = inference(model, data_file_path)\n",
    "        output_new_transposed_df = pd.DataFrame(output_new_transposed)\n",
    "        \n",
    "        X_pred = vae.predict(output_new_transposed_df.values, batch_size=16, verbose=0)\n",
    "        reconstruction_error = np.mean(np.square(output_new_transposed_df.values - X_pred), axis=1)\n",
    "        \n",
    "        threshold = np.percentile(reconstruction_error, 95)\n",
    "        anomalies = reconstruction_error > threshold\n",
    "        \n",
    "        anomalous_indices = np.where(anomalies)[0]\n",
    "        for index in anomalous_indices:\n",
    "            anomaly_counts[index] += 1\n",
    "    \n",
    "    # 获取被判定为异常次数最多的前15个数据点\n",
    "    top_anomalies = anomaly_counts.most_common(15)\n",
    "    top_anomalous_indices = [index for index, count in top_anomalies]\n",
    "\n",
    "    # 确认 'timestamp' 列存在并获取其值\n",
    "    if 'timestamp' not in data.columns:\n",
    "        print(\"No timestamp column found.\")\n",
    "        timestamp_values = ['N/A'] * len(top_anomalous_indices)\n",
    "    else:\n",
    "        timestamp_values = data.loc[top_anomalous_indices, 'timestamp'].values\n",
    "    \n",
    "    return top_anomalous_indices, timestamp_values\n",
    "\n",
    "# 超参数\n",
    "hidden_channels = 8  # 隐藏层维度\n",
    "heads = 2  # 多头注意力\n",
    "\n",
    "# 数据路径\n",
    "data_file_path = '/home/develop/Station/data/csv/test_station.csv'\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device('cpu')  # 强制使用 CPU\n",
    "\n",
    "# 初始化模型（这里假设模型的输入和输出维度与训练时相同）\n",
    "graph_data = prepare_data(data_file_path)\n",
    "in_channels = graph_data.x.size(1)\n",
    "out_channels = graph_data.x.size(1)  # 输出维度等于输入维度（重构任务）\n",
    "\n",
    "model = GATv2Net(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,  # 输出维度等于输入维度（重构任务）\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "# 构建VAE模型\n",
    "latent_dim = 20\n",
    "input_dim = graph_data.x.size(0)\n",
    "\n",
    "vae = build_vae(input_dim, latent_dim)\n",
    "\n",
    "# 执行多次推理并获取结果\n",
    "top_anomalous_indices, timestamp_values = run_inference_multiple_times(model, vae, data_file_path)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Top 15 anomalous indices and their 'timestamp' values:\")\n",
    "for index, timestamp in zip(top_anomalous_indices, timestamp_values):\n",
    "    print(f\"Index: {index}, timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been shuffled and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取目标文件\n",
    "file_path = '/home/develop/Station/data/csv/test_station.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 打散数据顺序\n",
    "shuffled_data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 保存打散后的数据\n",
    "shuffled_data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"The data has been shuffled and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 anomalous indices and their 'timestamp' values:\n",
      "Index: 12, timestamp: 2024-02-04T20:00:00\n",
      "Index: 20, timestamp: 2024-02-05T08:00:00\n",
      "Index: 49, timestamp: 2024-01-26T02:00:00\n",
      "Index: 51, timestamp: 2024-01-31T14:00:00\n",
      "Index: 76, timestamp: 2024-02-04T13:00:00\n",
      "Index: 93, timestamp: 2023-12-08T23:00:00\n",
      "Index: 111, timestamp: 2023-12-25T17:00:00\n",
      "Index: 178, timestamp: 2024-02-01T16:00:00\n",
      "Index: 30, timestamp: 2024-01-03T07:00:00\n",
      "Index: 158, timestamp: 2024-01-03T13:00:00\n",
      "Index: 78, timestamp: 2024-02-17T02:00:00\n",
      "Index: 86, timestamp: 2024-01-16T07:00:00\n",
      "Index: 104, timestamp: 2024-02-05T03:00:00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from collections import Counter\n",
    "\n",
    "# GATv2 模型定义\n",
    "class GATv2Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GATv2Net, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 移除 timestamp 列，如果存在的话\n",
    "    if 'timestamp' in data.columns:\n",
    "        features = data.drop(columns=['timestamp']).values\n",
    "    else:\n",
    "        features = data.values\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32).T \n",
    "    num_features = features.size(0)\n",
    "    \n",
    "    adj_matrix = torch.ones((num_features, num_features)) - torch.eye(num_features)  # 完全图\n",
    "    edge_index = dense_to_sparse(adj_matrix)[0]\n",
    "    \n",
    "    return Data(x=features, edge_index=edge_index)\n",
    "\n",
    "def inference(model, file_path):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    graph_data_new = prepare_data(file_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_new = model(graph_data_new.x, graph_data_new.edge_index)  # 获得模型输出\n",
    "\n",
    "    return output_new.T.numpy()  # 返回转置后的numpy数组\n",
    "\n",
    "# VAE相关部分\n",
    "def build_vae(input_dim, latent_dim):\n",
    "    class Encoder(layers.Layer):\n",
    "        def __init__(self, latent_dim, **kwargs):\n",
    "            super(Encoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(64, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(32, activation=\"relu\")\n",
    "            self.dense_mean = layers.Dense(latent_dim)\n",
    "            self.dense_log_var = layers.Dense(latent_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            z_mean = self.dense_mean(x)\n",
    "            z_log_var = self.dense_log_var(x)\n",
    "            return z_mean, z_log_var\n",
    "\n",
    "    class Decoder(layers.Layer):\n",
    "        def __init__(self, original_dim, **kwargs):\n",
    "            super(Decoder, self).__init__(**kwargs)\n",
    "            self.dense_1 = layers.Dense(32, activation=\"relu\")\n",
    "            self.dense_2 = layers.Dense(64, activation=\"relu\")\n",
    "            self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_1(inputs)\n",
    "            x = self.dense_2(x)\n",
    "            return self.dense_output(x)\n",
    "\n",
    "    class VAE(Model):\n",
    "        def __init__(self, original_dim, latent_dim, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.original_dim = original_dim\n",
    "            self.encoder = Encoder(latent_dim=latent_dim)\n",
    "            self.decoder = Decoder(original_dim=original_dim)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = self.encoder(inputs)\n",
    "            epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "            z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "            return self.decoder(z)\n",
    "\n",
    "    vae = VAE(original_dim=input_dim, latent_dim=latent_dim)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "    return vae\n",
    "\n",
    "def run_inference_multiple_times(model, vae, data_file_path, iterations=150):\n",
    "    anomaly_counts = Counter()\n",
    "    data = pd.read_csv(data_file_path)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        output_new_transposed = inference(model, data_file_path)\n",
    "        output_new_transposed_df = pd.DataFrame(output_new_transposed)\n",
    "        \n",
    "        X_pred = vae.predict(output_new_transposed_df.values, batch_size=16, verbose=0)\n",
    "        reconstruction_error = np.mean(np.square(output_new_transposed_df.values - X_pred), axis=1)\n",
    "        \n",
    "        threshold = np.percentile(reconstruction_error, 95)\n",
    "        anomalies = reconstruction_error > threshold\n",
    "        \n",
    "        anomalous_indices = np.where(anomalies)[0]\n",
    "        for index in anomalous_indices:\n",
    "            anomaly_counts[index] += 1\n",
    "    \n",
    "    # 获取被判定为异常次数最多的前15个数据点\n",
    "    top_anomalies = anomaly_counts.most_common(15)\n",
    "    top_anomalous_indices = [index for index, count in top_anomalies]\n",
    "\n",
    "    # 确认 'timestamp' 列存在并获取其值\n",
    "    if 'timestamp' not in data.columns:\n",
    "        print(\"No timestamp column found.\")\n",
    "        timestamp_values = ['N/A'] * len(top_anomalous_indices)\n",
    "    else:\n",
    "        timestamp_values = data.loc[top_anomalous_indices, 'timestamp'].values\n",
    "    \n",
    "    return top_anomalous_indices, timestamp_values\n",
    "\n",
    "# 超参数\n",
    "hidden_channels = 8  # 隐藏层维度\n",
    "heads = 2  # 多头注意力\n",
    "\n",
    "# 数据路径\n",
    "data_file_path = '/home/develop/Station/data/csv/test_station.csv'\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device('cpu')  # 强制使用 CPU\n",
    "\n",
    "# 初始化模型（这里假设模型的输入和输出维度与训练时相同）\n",
    "graph_data = prepare_data(data_file_path)\n",
    "in_channels = graph_data.x.size(1)\n",
    "out_channels = graph_data.x.size(1)  # 输出维度等于输入维度（重构任务）\n",
    "\n",
    "model = GATv2Net(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,  # 输出维度等于输入维度（重构任务）\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "# 构建VAE模型\n",
    "latent_dim = 20\n",
    "input_dim = graph_data.x.size(0)\n",
    "\n",
    "vae = build_vae(input_dim, latent_dim)\n",
    "\n",
    "# 执行多次推理并获取结果\n",
    "top_anomalous_indices, timestamp_values = run_inference_multiple_times(model, vae, data_file_path)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Top 15 anomalous indices and their 'timestamp' values:\")\n",
    "for index, timestamp in zip(top_anomalous_indices, timestamp_values):\n",
    "    print(f\"Index: {index}, timestamp: {timestamp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
